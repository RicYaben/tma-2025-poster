{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118971d0",
   "metadata": {},
   "source": [
    "# Rolling the DICE: \n",
    "## A Device Identification and Classification Engine to detect vulnerable devices facing the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41d279e",
   "metadata": {},
   "source": [
    "This repository contains an example implementation of DICE's classifier and identifier modules for 8 common protocols in IoT and OT.\n",
    "As proof of work, this code does not represent the final product, and only provides an understanding of how scanning records, identifiers and classifiers work together.\n",
    "The content of this repository was submited to the Network Traffic Measurement and Analysis (TMA) conference to support a poster with the same title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb24067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from datetime import datetime, timezone\n",
    "from cryptography import x509\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from packaging.version import parse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "import os\n",
    "import base64\n",
    "import struct\n",
    "import re\n",
    "import itertools\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f4a29",
   "metadata": {},
   "source": [
    "DICE's structure is divided into three components: scanner, classifier, and identifier.\n",
    "Each component is composed of one or more (sometimes) linked modules.\n",
    "This notebook includes example implementations to identifier and classifier modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(ABC):\n",
    "    def label(self, row: pd.Series) -> list[str]: ...\n",
    "    \n",
    "class Identifier(ABC):\n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series: ...\n",
    "\n",
    "class Signature(ABC):\n",
    "    def classify(self, id, tag): ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82940bb",
   "metadata": {},
   "source": [
    "This minimal example exists to show how classifiers can be linked together to work with common data.\n",
    "Here, DICE only implements one command, `classify`, which takes a dataset and a singature (a set of identifiers and classifiers).\n",
    "The signature acts on each of the records as if they were `scan` events, fingerprints them, and assign classification labels.\n",
    "In this example there will be only one identifier per signature.\n",
    "\n",
    "The implementation we are aiming to cover should match the following command:\n",
    "\n",
    "```bash\n",
    "dice classify --signatures * -f zgrab2_* -s zgrab2\n",
    "```\n",
    "\n",
    "This command makes DICE classify previously gathered records using ZGrab2 `-f zgrab2_* -s zgrab2`. Then, `--signatures *` represents a blob, where we are trying to load all existing signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e0e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DICE:\n",
    "    def classify(df: pd.DataFrame, fp: Identifier, clss: list[Classifier]) -> pd.DataFrame:\n",
    "        df_post = df.apply(fp.fingerprint, axis=1)\n",
    "        df_post[\"labels\"] = df_post.apply(lambda row: list(itertools.chain(*[cls.label(row) for cls in clss])), axis=1)\n",
    "        return df_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "class DatasetLoader(Protocol):\n",
    "    def __call__(self, scan_id: str, protocol: str) -> pd.DataFrame: ...\n",
    "\n",
    "class DatasetStore:\n",
    "    root_path: str = \"data\"\n",
    "\n",
    "    def normalize_df(self, df:pd.DataFrame, protocol:str) -> pd.DataFrame:\n",
    "        def explode(data: dict) -> pd.Series:\n",
    "            proto_data = data.get(protocol)\n",
    "            cols = [\"error\", \"protocol\", \"result\", \"status\", \"timestamp\"]\n",
    "            return pd.Series({key:proto_data.get(key, None) for key in cols})\n",
    "        \n",
    "        ndf = pd.DataFrame(df[\"data\"].apply(explode))\n",
    "        norm = (\n",
    "            pd.concat([df,ndf], axis=1)\n",
    "            .drop(columns=[\"data\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        norm['timestamp'] = pd.to_datetime(norm['timestamp'])\n",
    "        return norm\n",
    "\n",
    "    def to_parquet(self, scan_id: str, protocol: str, schema: pa.Schema, dfs, prefix: str = None, separator: str = '_'):\n",
    "        # filepath and filename\n",
    "        fpath = os.path.join(self.root_path, scan_id, f'protocol={protocol}')\n",
    "        fname = separator.join(filter(None, [prefix, protocol]))\n",
    "\n",
    "        # Store dfs to parquet\n",
    "        # NOTE: the writer truncates the file!\n",
    "        with pq.ParquetWriter(os.path.join(fpath, f'{fname}.parquet'), schema) as writer:\n",
    "            for df in dfs:\n",
    "                df = df.reindex(columns=schema.names)\n",
    "                table = pa.Table.from_pandas(df, schema=schema)\n",
    "                writer.write_table(table=table)\n",
    "\n",
    "    def read_parquet(self, scan_id: str, protocol: str) -> pd.DataFrame:\n",
    "        fpath = os.path.join(self.root_path, scan_id, f'protocol={protocol}', f'{protocol}.parquet')\n",
    "        df = pd.read_parquet(fpath, engine='pyarrow')\n",
    "        ndf = (\n",
    "            pd.json_normalize(df[\"result\"])\n",
    "            .rename(columns=lambda x: x.removeprefix(\"result.\"))\n",
    "        )\n",
    "        df = (\n",
    "            pd.concat([df, ndf], axis=1)\n",
    "            .drop(columns=[\"result\"])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Remove duplicated rows based on the IP and status\n",
    "        df = df.drop_duplicates(subset=[\"ip\", \"status\"])\n",
    "        return df\n",
    "\n",
    "class Results:\n",
    "    id = \"\"\n",
    "    protocol = \"\"\n",
    "    df = None\n",
    "\n",
    "    def __init__(self, id: str, protocol: str, cls_df: pd.DataFrame):\n",
    "        self.id = id\n",
    "        self.protocol = protocol\n",
    "        self.df = cls_df\n",
    "\n",
    "    def summary(self) -> dict:\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'protocol': self.protocol,\n",
    "            'flagged': len(self.df.index),\n",
    "            'classified': len(self.df[self.df.labels.astype(bool)].index)\n",
    "        }\n",
    "    \n",
    "    def labels(self) -> pd.DataFrame:\n",
    "        self.df[\"labels\"] = self.df.labels.apply(set)\n",
    "        exploded_df = self.df.explode('labels')\n",
    "        return exploded_df[\"labels\"].value_counts()\n",
    "    \n",
    "    def to_parquet(self, prefix: str = \"dice\", schema: pa.Schema | None = None, mode: str = 'labelled'):\n",
    "        df = self.df\n",
    "\n",
    "        if mode == \"labelled\":\n",
    "            df = df[df.labels.astype(bool)]\n",
    "            \n",
    "        if not schema:\n",
    "            schema = pa.Schema.from_pandas(self.df)\n",
    "\n",
    "        ds.to_parquet(\n",
    "            scan_id=self.id,\n",
    "            protocol=self.protocol,\n",
    "            schema=schema,\n",
    "            dfs=[self.df],\n",
    "            prefix=prefix,\n",
    "        )\n",
    "\n",
    "class Signature:\n",
    "    idf = None\n",
    "    classifiers = []\n",
    "    filter = None\n",
    "    loader = None\n",
    "\n",
    "    def set_classifiers(self, clss: Classifier | list[Classifier]):\n",
    "        if isinstance(clss, list):\n",
    "            self.classifiers = clss\n",
    "        else:\n",
    "            self.classifiers = [clss]\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def set_identifier(self, idf: Identifier):\n",
    "        self.idf = idf\n",
    "        return self\n",
    "\n",
    "    def set_filter(self, f):\n",
    "        self.filter = f\n",
    "        return self\n",
    "    \n",
    "    def set_loader(self, loader):\n",
    "        self.loader = loader\n",
    "        return self\n",
    "    \n",
    "    def classify(self, id: str, protocol: str) -> Results:\n",
    "        df = self.loader(scan_id=id, protocol=protocol)\n",
    "        df = self.filter(df)\n",
    "\n",
    "        return Results(\n",
    "            id=id, \n",
    "            protocol=protocol,\n",
    "            cls_df=DICE.classify(df, self.idf, self.classifiers)\n",
    "        )\n",
    "\n",
    "ds = DatasetStore()\n",
    "\n",
    "def classify_records(\n",
    "        id: str, \n",
    "        protocol: str,\n",
    "        filter: callable,\n",
    "        fp: Identifier, \n",
    "        clss: list[Classifier],\n",
    "        loader: DatasetLoader = ds.read_parquet\n",
    "    ) -> 'Results':\n",
    "\n",
    "    signature = Signature() \\\n",
    "        .set_filter(filter) \\\n",
    "        .set_identifier(fp) \\\n",
    "        .set_classifiers(clss) \\\n",
    "        .set_loader(loader)\n",
    "    \n",
    "    return signature.classify(id, protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a6265",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c359481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "def parse_certificate(certificate: str) -> dict:\n",
    "    \"parses a X509 certificate\"\n",
    "\n",
    "    header =  \"-----BEGIN CERTIFICATE-----\"\n",
    "    if not certificate.startswith(header):\n",
    "        certificate = \"%s\\n%s\" % (header, certificate)\n",
    "\n",
    "    footer = \"-----END CERTIFICATE-----\"\n",
    "    if (not certificate.endswith(footer)) or (not certificate.endswith(f\"{footer}\\n\")):\n",
    "        certificate = \"%s\\n%s\\n\" % (certificate, footer)\n",
    "\n",
    "    b = certificate.encode()\n",
    "    try:\n",
    "        cert = x509.load_pem_x509_certificate(b)\n",
    "\n",
    "        pkey = None\n",
    "        err = None\n",
    "        try:\n",
    "            pkey = cert.public_key()\n",
    "        except Exception as e:\n",
    "            err = e\n",
    "\n",
    "        not_before = cert.not_valid_before_utc.replace(tzinfo=timezone.utc)\n",
    "        not_after = cert.not_valid_after_utc.replace(tzinfo=timezone.utc)\n",
    "\n",
    "        return dict(\n",
    "            raw=certificate,\n",
    "            subject=cert.subject,\n",
    "            issuer=cert.issuer,\n",
    "            pkey=pkey,\n",
    "            signature_algorithm=cert.signature_algorithm_oid._name,\n",
    "            not_before=not_before,\n",
    "            not_after=not_after,\n",
    "            error=err\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Ignore, bad certificate\n",
    "        return {\n",
    "            'raw': certificate,\n",
    "            'error': e\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9f4f0",
   "metadata": {},
   "source": [
    "### Certificates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CertificateClassifier(Classifier):\n",
    "    '''validate certificates'''\n",
    "    seen_certs = []\n",
    "    raw_keys: list[bytes] = []\n",
    "\n",
    "    labels = dict(\n",
    "        # Validity\n",
    "        expired=\"certificate:validity:expired\",\n",
    "        negative=\"certificate:validity:negative\",\n",
    "        long=\"certificate:validity:long-lasting\",\n",
    "        future=\"certificate:validity:future\",\n",
    "        # Encryption\n",
    "        weak_hash=\"certificate:hash:weak\",\n",
    "        weak_encryption=\"certificate:encryption:weak\",\n",
    "        # Cert\n",
    "        reused=\"certificate:reused\",\n",
    "        # Public Key\n",
    "        short_key=\"certificate:key:short\",\n",
    "        reused_key=\"certificate:key:reused\"\n",
    "    )\n",
    "    \n",
    "    def eval_times(self, cert, timestamp: datetime) -> list[str]:\n",
    "        labs = []\n",
    "\n",
    "        not_before = cert.get(\"not_before\")\n",
    "        not_after = cert.get(\"not_after\")\n",
    "\n",
    "        if not (not_before and not_after):\n",
    "            return labs \n",
    "\n",
    "        if (not_before > timestamp):\n",
    "            labs.append(self.labels.get(\"future\"))\n",
    "\n",
    "        if (not_after < timestamp):\n",
    "            labs.append(self.labels.get(\"expired\"))\n",
    "\n",
    "        if ((not_after - not_before) <= pd.Timedelta(days=0)):\n",
    "            labs.append(self.labels.get(\"negative\"))\n",
    "\n",
    "        if (not_after - not_before) > pd.Timedelta(days=20*365.25):\n",
    "            labs.append(self.labels.get(\"long\"))\n",
    "        return labs\n",
    "    \n",
    "    def eval_crypto(self, cert) -> list[str]:\n",
    "        labs=[]\n",
    "\n",
    "        # signature, e.g., sha256WithRSAEncryption\n",
    "        # includes the hash function and the encryption algorithm names\n",
    "        signature: str = cert.get(\"signature_algorithm\", \"\")\n",
    "        if not signature:\n",
    "            return labs\n",
    "\n",
    "        separator = \"With\" if \"With\" in signature else \"-with-\"\n",
    "        hash_func = signature.split(separator)[0]\n",
    "\n",
    "        # Check hash function\n",
    "        # SHA-1: deprecated in 2011\n",
    "        if hash_func in [\"md5\", \"sha1\", \"dsa\"]:\n",
    "            labs.append(self.labels.get(\"weak_hash\"))\n",
    "\n",
    "        return labs\n",
    "    \n",
    "    def eval_key(self, cert) -> list[str]:\n",
    "        labs = []\n",
    "        key = cert.get(\"pkey\")\n",
    "        if pd.isna(key):\n",
    "            return labs\n",
    "\n",
    "        if key.key_size < 2048:\n",
    "            labs.append(self.labels.get(\"short_key\"))\n",
    "\n",
    "        raw_key = key.public_bytes(\n",
    "            encoding=serialization.Encoding.DER,\n",
    "            format=serialization.PublicFormat.SubjectPublicKeyInfo\n",
    "        )\n",
    "        if raw_key in self.raw_keys:\n",
    "            labs.append(self.labels.get(\"reused_key\"))\n",
    "        self.raw_keys.append(raw_key)\n",
    "        return labs\n",
    "    \n",
    "    def eval_certificate(self, cert, timestamp) -> list[str]:\n",
    "        labs = []\n",
    "        if pd.isna(cert) or not cert.get(\"raw\"):\n",
    "            return labs\n",
    "\n",
    "        if l := self.eval_times(cert, timestamp):\n",
    "            labs.extend(l)\n",
    "\n",
    "        if l := self.eval_crypto(cert):\n",
    "            labs.extend(l)\n",
    "\n",
    "        if l := self.eval_key(cert):\n",
    "            labs.extend(l)\n",
    "\n",
    "        if (raw:=cert.get(\"raw\")) in self.seen_certs:\n",
    "            labs.append(self.labels.get(\"reused\"))\n",
    "        else:\n",
    "            self.seen_certs.append(raw)\n",
    "\n",
    "        return labs\n",
    "\n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        cert = row[\"fingerprint\"][\"certificate\"]\n",
    "        timestamp = row[\"timestamp\"]\n",
    "        return self.eval_certificate(cert, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acab4b",
   "metadata": {},
   "source": [
    "### BacNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BacNetFingerprinter(Identifier):\n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        row[\"fingerprint\"] = {\n",
    "            \"vendor_name\": row.get(\"vendor_name\"),\n",
    "            \"model_name\": row.get(\"model_name\"),\n",
    "            \"firmware_revision\": row.get(\"firmware_revision\")\n",
    "        }\n",
    "        return row\n",
    "    \n",
    "class BacNetClassifier(Classifier):\n",
    "    def eval_access(self, row) -> list[str]:\n",
    "        labs = []\n",
    "        # Any of the fields would work\n",
    "        fp = row.get(\"fingerprint\")\n",
    "        if any(pd.notna(l) for l in fp.values()):\n",
    "            labs.append(\"bacnet:access:read\")\n",
    "        return labs\n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        labs = []\n",
    "        if l := self.eval_access(row):\n",
    "            labs.extend(l)\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18087ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bacnet_filter(df): return df[df.is_bacnet.astype(bool)]\n",
    "bacnet_ds = classify_records(\n",
    "    'tma-2025', \n",
    "    'bacnet',\n",
    "    bacnet_filter,\n",
    "    BacNetFingerprinter(),\n",
    "    BacNetClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1ea68",
   "metadata": {},
   "source": [
    "### CoAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb6af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAPIdentifier(Identifier):\n",
    "    def get_str_rep(self, path):\n",
    "        msgs = path.get(\"messages\", [])\n",
    "        if len(msgs) == 0:\n",
    "            return\n",
    "        \n",
    "        payload = msgs[0].get(\"payload\")\n",
    "        if not payload:\n",
    "            return\n",
    "        return payload.get(\"StringRep\", \"\")\n",
    "\n",
    "    def get_capabilities(self, path):\n",
    "        caps = []\n",
    "        if rep := self.get_str_rep(path):\n",
    "            matches = re.findall(r'<([^>]+)>(?:;([^=]+)=\"([^\"]*)\")*', rep)\n",
    "            for match in matches:\n",
    "                resource_path = match[0]\n",
    "                # NOTE: This one does not work, attributes are empty in all cases\n",
    "                # TODO FIXME: transform the attributes to an object, and put it as a string\n",
    "                attributes = re.findall(r'([^=]+)=\"([^\"]*)\"', \";\".join(match[1:]))\n",
    "                caps.append([(\"path\", resource_path), (\"attributes\", attributes)])\n",
    "        return caps\n",
    "    \n",
    "    def get_home_payload(self, path):\n",
    "        return self.get_str_rep(path)\n",
    "\n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        endpoints = row.get(\"result\", [])\n",
    "        fp = {}\n",
    "\n",
    "        # in reality we only have 2 endpoints, the core and the home,\n",
    "        # the core goes first, and then the home.\n",
    "        for ep in endpoints:\n",
    "            match ep.get(\"path\"):\n",
    "                case \".well-known/core\":\n",
    "                    fp[\"capabilities\"] = self.get_capabilities(ep)\n",
    "                case \"/\":\n",
    "                    fp[\"payload\"] = self.get_home_payload(ep)\n",
    "        row['fingerprint'] = fp\n",
    "        return row\n",
    "    \n",
    "class CoAPClassifier:\n",
    "\n",
    "    def eval_payload_libcoap(self, p):\n",
    "        year = re.search(r\"\\d+--(\\d+)\", p)\n",
    "        if not year:\n",
    "            return\n",
    "        \n",
    "        # lower than v4.3 is vulnerable\n",
    "        # https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query=libcoap&search_type=all&isCpeNameSearch=true\n",
    "        # cpe:2.3:a:libcoap:libcoap:*:*:*:*:*:*:*:*\n",
    "        if int((y := year.group(1))) < 2023:\n",
    "            return f\"coap:product:libcoap:{y}\"\n",
    "\n",
    "    def eval_payload_californium(self, p):\n",
    "        # Anything below 3.13.0 is not supported, released in 2024\n",
    "        copyright = re.search(r\"\\(c\\)(.*?)Institute for Pervasive Computing\", p)\n",
    "        if not copyright:\n",
    "            return\n",
    "        \n",
    "        copyright = copyright.group(1)\n",
    "        numbers = list(map(int, re.findall(r\"\\d+\", copyright)))\n",
    "        if not any(num >= 2023 for num in numbers):\n",
    "            return \"coap:product:eclipse-californium:outdated\"\n",
    "\n",
    "    def eval_payload(self, payload: str):\n",
    "        labs = []\n",
    "        if pd.isna(payload):\n",
    "            return labs\n",
    "        \n",
    "        match payload:\n",
    "            case p if \"libcoap\" in payload:\n",
    "                if l := self.eval_payload_libcoap(p):\n",
    "                    labs.append(l)\n",
    "\n",
    "            case p if \"Eclipse Californium\" in payload:\n",
    "                if l := self.eval_payload_californium(p):\n",
    "                    labs.append(l)\n",
    "        return labs\n",
    "            \n",
    "    def eval_capabilities(self, caps: list):\n",
    "        labs = []\n",
    "        if (isinstance(caps, np.ndarray) and caps.size > 0) or (isinstance(caps, list) and len(caps)):\n",
    "            labs.append('coap:access:read')\n",
    "        return labs\n",
    "\n",
    "    def eval_fingerprint(self, fingerprint: dict):\n",
    "        labs = []\n",
    "\n",
    "        if l := self.eval_capabilities(fingerprint.get(\"capabilities\")):\n",
    "            labs.extend(l)\n",
    "\n",
    "        if l := self.eval_payload(fingerprint.get(\"payload\")):\n",
    "            labs.extend(l)\n",
    "        return labs\n",
    "\n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        labs = []\n",
    "        fp = row.get('fingerprint')\n",
    "        if l := self.eval_fingerprint(fp):\n",
    "            labs.extend(l)\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af217263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coap_loader(scan_id, protocol):\n",
    "    fpath = os.path.join('data', scan_id ,f'protocol={protocol}', f'{protocol}.parquet')\n",
    "    df = pd.read_parquet(fpath, engine='pyarrow')\n",
    "    df = df.drop_duplicates(subset=[\"ip\", \"status\"])\n",
    "    return df\n",
    "\n",
    "def coap_filter(df): return df[df.result.notna()]\n",
    "coap_ds = classify_records(\n",
    "    \"tma-2025\",\n",
    "    \"coap\",\n",
    "    coap_filter,\n",
    "    CoAPIdentifier(),\n",
    "    CoAPClassifier(),\n",
    "    coap_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100af41",
   "metadata": {},
   "source": [
    "### DNP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2329d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNP3Fingerprinter(Identifier):\n",
    "    def map_function_code(self, primary, fcode):\n",
    "        if primary == 1:\n",
    "            match fcode:\n",
    "                case 0:\n",
    "                    return \"RESET_LINK_STATES\"\n",
    "                case 4:\n",
    "                    return \"UNCONFIRMED_USER_DATA\"\n",
    "                case 9:\n",
    "                    return \"REQUEST_LINK_STATUS\"\n",
    "        if primary == 0:\n",
    "            match fcode:\n",
    "                case 0:\n",
    "                    return \"ACK\"\n",
    "                case 11:\n",
    "                    return \"LINK_STATUS\"\n",
    "                case 15:\n",
    "                    return \"NOT_SUPPORTED\"\n",
    "        return \"UNKNOWN\"\n",
    "    \n",
    "    def parse_data_link_layer(self, buffer: bytes) -> dict | None:\n",
    "        # Error: payload too small to contain a valid DNP3 frame\n",
    "        if len(buffer) < 10:\n",
    "            return\n",
    "        \n",
    "        # Decode data link layer (little-endian)\n",
    "        start, length, control, destination, source, crc = struct.unpack(\"<HBBHHH\", buffer[:10])\n",
    "\n",
    "        # Error: invalid start of frame marker\n",
    "        if start != 0x6405:\n",
    "            return\n",
    "        \n",
    "        control_header= dict(\n",
    "            direction=(control >> 7) & 0x01, # Bit 7\n",
    "            primary=(control >> 6) & 0x01, # Bit 6\n",
    "            fcb_fcv_or_dfc = (control >> 4) & 0x03, # Bits 5-4\n",
    "            function_code = control & 0x0F, # Bits 3-0\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            start=hex(start),\n",
    "            length=length,\n",
    "            control=control_header,\n",
    "            destination=destination,\n",
    "            source=source,\n",
    "            crc=hex(crc),\n",
    "            function_name=self.map_function_code(control_header[\"primary\"], control_header[\"function_code\"]),\n",
    "        )\n",
    "    \n",
    "    def parse_frame(self, buffer: bytes) -> tuple[dict, bytes]:\n",
    "        HEADER_LENGTH: int = 5\n",
    "        HEADER_START: int = 10\n",
    "\n",
    "        frame={}\n",
    "        dlink_layer = self.parse_data_link_layer(buffer)\n",
    "        if not dlink_layer:\n",
    "            return None, None\n",
    "        \n",
    "        frame[\"data_link_layer\"] = dlink_layer\n",
    "        frame_size = dlink_layer[\"length\"] + HEADER_LENGTH\n",
    "        frame_data = buffer[:frame_size]\n",
    "        \n",
    "        if dlink_layer[\"length\"] > HEADER_LENGTH and len(frame_data) > HEADER_START:\n",
    "            HEADER_START += 1\n",
    "            frame[\"transport_layer\"] = dict(\n",
    "                transport_control=bin(frame_data[HEADER_START]),\n",
    "            )\n",
    "\n",
    "        app_layer=frame_data[HEADER_START:]\n",
    "        frame[\"application_layer\"] = app_layer\n",
    "\n",
    "        return (frame, buffer[frame_size:])\n",
    "\n",
    "    def parse_response_frames(self, response: str) -> list[dict]:\n",
    "        frames = []\n",
    "        if not response:\n",
    "            return frames\n",
    "\n",
    "        # A DNP3 response comes encoded as base64, so the first thing is to convert it to a byte string\n",
    "        buffer = base64.b64decode(response)\n",
    "        while buffer:\n",
    "            frame, buffer = self.parse_frame(buffer)\n",
    "            if not frame:\n",
    "                break\n",
    "            \n",
    "            frames.append(frame)\n",
    "        return frames\n",
    "    \n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        frames = self.parse_response_frames(row.raw_response)\n",
    "        row['fingerprint'] = {\"frames\":frames}\n",
    "        return row\n",
    "    \n",
    "class DNP3Classifier(Classifier):\n",
    "    def is_unexpected(self, frames: list) -> bool:\n",
    "        '''\n",
    "        Some responses may contian only frames with request link status codes\n",
    "        This is an unexpected behavior.\n",
    "        '''\n",
    "        def fn_req_link(frame):\n",
    "            dlink = frame.get(\"data_link_layer\")\n",
    "            if dlink and dlink.get(\"function_name\") == \"REQUEST_LINK_STATUS\":\n",
    "                return True\n",
    "\n",
    "        reqs = filter(fn_req_link, frames)\n",
    "        return len(list(reqs)) > 0\n",
    "\n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        labs = []\n",
    "        fp = row.get(\"fingerprint\")\n",
    "        frames = fp.get(\"frames\", [])\n",
    "        if not frames:\n",
    "            return labs\n",
    "\n",
    "        # Check is an echo response\n",
    "        if self.is_unexpected(frames):\n",
    "            return labs\n",
    "        labs.append(\"dnp3:access:read\")\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnp3_filter(df): \n",
    "    return df[df.is_dnp3.astype(bool) & df.raw_response.notna()]\n",
    "\n",
    "dnp_ds = classify_records(\n",
    "    f'tma-2025',\n",
    "    'dnp3',\n",
    "    dnp3_filter,\n",
    "    DNP3Fingerprinter(), \n",
    "    DNP3Classifier(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497fc4b",
   "metadata": {},
   "source": [
    "### Modbus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModbusIdentifier(Identifier):\n",
    "    def get_object(self, objects, key):\n",
    "        if not objects or objects is np.nan:\n",
    "            return None\n",
    "        \n",
    "        for k, v in objects:\n",
    "            if k == key: return str(v).strip() if v else None\n",
    "\n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        objs = row.get(\"mei_response.objects\", [])\n",
    "        row[\"fingerprint\"] = dict(\n",
    "            vendor=self.get_object(objs, \"vendor\"),\n",
    "            product_code=self.get_object(objs, \"product_code\"),\n",
    "            version=self.get_object(objs, \"revision\")\n",
    "        )\n",
    "        return row\n",
    "    \n",
    "class ModbusClassifier(Classifier):\n",
    "    def label(self, row: pd.Series):\n",
    "        labs = []\n",
    "        if row[\"fingerprint\"][\"vendor\"]:\n",
    "            labs.extend(['modbus:access:anonymous'])\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faefa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modbus_filter(df):\n",
    "    return df[(df.status == 'success') & (df.mei_response.astype(bool))]\n",
    "\n",
    "modbus_ds = classify_records(\n",
    "    f'tma-2025',\n",
    "    'modbus',\n",
    "    modbus_filter,\n",
    "    ModbusIdentifier(), \n",
    "    ModbusClassifier(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97119290",
   "metadata": {},
   "source": [
    "### MQTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ebac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MQTTIdentifier(Identifier):\n",
    "    def find_hub_info(self, topics):\n",
    "        name, version = (None, None)\n",
    "        for topic, values in topics:\n",
    "            if name and version:\n",
    "                return (name, version)\n",
    "            \n",
    "            val = values[0]\n",
    "            match topic:\n",
    "                case t if topic.endswith('/sysdescr'):\n",
    "                    name = val\n",
    "                    continue\n",
    "\n",
    "                case t if topic.endswith('/version'):\n",
    "                    version = val\n",
    "        return (name, version)\n",
    "\n",
    "    def get_broker(self, row: pd.Series) -> dict:\n",
    "        # NOTE: Homeassistants and bridges are not brokers, these are separate\n",
    "        # devices using the broker to communicate. Do not confuse them\n",
    "        broker = {\n",
    "            \"broker\": None,\n",
    "            \"version\": None,\n",
    "            \"cpe\": None,\n",
    "            \"topics\": row.get(\"topics\", []),\n",
    "        }\n",
    "\n",
    "        if not broker['topics']:\n",
    "            return broker\n",
    "        \n",
    "        for topic, values in (topics := broker['topics']):\n",
    "            match topic:\n",
    "                # hub\n",
    "                case \"$SYS/brokers\":\n",
    "                    broker[\"broker\"], broker[\"version\"] = self.find_hub_info(topics)\n",
    "                    return broker\n",
    "                # case-specific\n",
    "                case s if s.startswith(\"$SYS/VerneMQ\"):\n",
    "                    broker[\"broker\"] = \"VerneMQ\"\n",
    "                    broker[\"cpe\"] = \"cpe:2.3:a:octavolabs:vernemq:*:*:*:*:*:*:*:*\"\n",
    "                    return broker\n",
    "                case s if s.startswith((\"$SYS/ActiveMQ\", \"ActiveMQ/\")):\n",
    "                    broker[\"broker\"] = \"ActiveMQ\"\n",
    "                    broker[\"cpe\"] = \"cpe:2.3:a:apache:activemq:*:*:*:*:*:*:*:*\"\n",
    "                    return broker\n",
    "                \n",
    "                # regular mqtt broker\n",
    "                case \"$SYS/broker/version\":\n",
    "                    version = values[0] if len(values) else \"\"\n",
    "                    # mosquitto\n",
    "                    if \"mosquitto\" in version:\n",
    "                        broker[\"broker\"] = \"mosquitto\"\n",
    "                        broker[\"version\"] = parse(version.split(\"mosquitto version\")[1])\n",
    "                        broker[\"cpe\"] = \"cpe:2.3:a:eclipse:mosquitto:*:*:*:*:*:*:*:*\"\n",
    "                        return broker\n",
    "\n",
    "                    try:\n",
    "                        # unknown, we only know the version\n",
    "                        b = version.split(\"version\")\n",
    "                        version = parse(b[-1])\n",
    "                        broker[\"broker\"] = b[0]\n",
    "                    except Exception:\n",
    "                        broker[\"broker\"] = version\n",
    "                        broker[\"version\"] = None\n",
    "\n",
    "                    return broker\n",
    "                \n",
    "        return broker\n",
    "    \n",
    "    def get_fingerprint(self, row):\n",
    "        cert = None\n",
    "        if (c := row.get(\"certificate\", None)) is not None:\n",
    "            cert = parse_certificate(c[0])\n",
    "\n",
    "        fp = self.get_broker(row)\n",
    "        fp.update({'certificate': cert})\n",
    "        return fp\n",
    "\n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        row[\"fingerprint\"] = self.get_fingerprint(row)\n",
    "        return row\n",
    "    \n",
    "class MQTTClassifier(Classifier):\n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        labs = []\n",
    "        match row.get(\"scheme\"):\n",
    "            case \"tcp\":\n",
    "                labs.append(\"mqtt:access:anonymous\")\n",
    "            case \"ssl\":\n",
    "                labs.append(\"mqtt:access:self-signed-certificate\")\n",
    "\n",
    "        fp = row.get(\"fingerprint\")\n",
    "        topics = fp.get(\"topics\", [])\n",
    "        if topics:\n",
    "            labs.append(\"mqtt:access:read\") \n",
    "\n",
    "        for topic, _ in fp.get(\"topics\", []):\n",
    "            if topic.startswith(\"$SYS/\"):\n",
    "                labs.append(\"mqtt:access:read-internal\")\n",
    "                break\n",
    "        return labs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28430a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mqtt_filter_topics(df): return df[df.topics.astype(bool)]\n",
    "mqtt_ds = classify_records(\n",
    "    'tma-2025',\n",
    "    'mqtt',\n",
    "    mqtt_filter_topics,\n",
    "    MQTTIdentifier(),\n",
    "    [MQTTClassifier(), CertificateClassifier()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791c7c2",
   "metadata": {},
   "source": [
    "### OPC UA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPCUAIdentifier(Identifier):\n",
    "    def parse_security_policy_URI(self, uri, default=\"None\") -> str:\n",
    "        if len(sp := uri.split(\"SecurityPolicy#\")) > 1:\n",
    "            return sp[-1]\n",
    "        return default\n",
    "\n",
    "    def get_security_policies(self, endpoint) -> list[tuple[str, str]]:\n",
    "        ttype = {\n",
    "            0: \"ANONYMOUS_0\",\n",
    "            1: \"USERNAME_1\",\n",
    "            2: \"CERTIFICATE_2\",\n",
    "            3: \"ISSUEDTOKEN_3\",\n",
    "        }\n",
    "\n",
    "        default_policy = self.parse_security_policy_URI(endpoint[\"SecurityPolicyURI\"])\n",
    "\n",
    "        # Some descriptors do not have identity tokens,\n",
    "        # this value is stored as None and it exists in the object\n",
    "        utokens = endpoint.get(\"UserIdentityTokens\")\n",
    "        if utokens is None:\n",
    "            return []\n",
    "\n",
    "        policies = []\n",
    "        for utoken in utokens:\n",
    "            spolicy = self.parse_security_policy_URI(utoken[\"SecurityPolicyURI\"], default_policy)\n",
    "            ttoken = ttype.get(utoken[\"TokenType\"], None)\n",
    "            policies.append((ttoken, spolicy))\n",
    "        return policies\n",
    "    \n",
    "    def get_system_info(self, nodes) -> dict:\n",
    "        build_nodes = list(filter(lambda x: x[\"browse_name\"] in [\"ServerStatus\", \"BuildInfo\"], nodes))\n",
    "        for node in build_nodes:\n",
    "            match node[\"browse_name\"]:\n",
    "                case \"ServerStatus\":\n",
    "                    if v := node.get(\"value\"):\n",
    "                        return v[\"BuildInfo\"]\n",
    "                case \"BuildInfo\":\n",
    "                    if v := node.get(\"value\"):\n",
    "                        return v\n",
    "                \n",
    "        # Otherwise try to figure out the manufacturer and other info\n",
    "        info_names = [\n",
    "            \"ProductURI\", \n",
    "            \"ManufacturerName\", \n",
    "            \"ProductName\", \n",
    "            \"SoftwareVersion\", \n",
    "            \"BuildNumber\", \n",
    "            \"BuildDate\",\n",
    "            \"SerialNumber\",\n",
    "            # alternatives\n",
    "            \"Model\",\n",
    "            \"SoftwareRevision\",\n",
    "            \"DeviceRevision\",\n",
    "            \"HardwareRevision\"\n",
    "        ]\n",
    "        sys_info = {}\n",
    "        for node in nodes:\n",
    "            if n := node[\"browse_name\"] in info_names:\n",
    "                sys_info[n] = node.get(\"value\")\n",
    "        return sys_info\n",
    "\n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        ends = []\n",
    "        nodes = n if pd.notna(n := row.get(\"nodes\")) else {}\n",
    "        if np.all(pd.notnull(eps := row.get(\"endpoints\", []))):\n",
    "            for endpoint in eps:\n",
    "                ends.append({\n",
    "                    \"endpoint\": endpoint,\n",
    "                    \"info\": self.get_system_info(nodes.get(endpoint.get(\"EndpointURL\"), [])),\n",
    "                    \"policies\": self.get_security_policies(endpoint),\n",
    "                    \"mode\": endpoint.get(\"SecurityMode\"),\n",
    "                    \"certificate\": parse_certificate(cert) if pd.notna(cert := endpoint.get(\"ServerCertificate\")) else None\n",
    "                })\n",
    "\n",
    "        row[\"fingerprint\"] = dict(\n",
    "            endpoints=ends\n",
    "        )\n",
    "        return row\n",
    "\n",
    "class OPCUAClassifier(Classifier):\n",
    "    def eval_policy(self, policies, mode):\n",
    "        labs = []\n",
    "        # Anonymous + None = no access control\n",
    "        if (\"ANONYMOUS_0\", \"None\") in policies:\n",
    "            labs.append(\"opcua:access:anonymous\")\n",
    "\n",
    "        # broken signing and encryptions\n",
    "        atuhs = set([auth for _, auth in policies])\n",
    "        if set([\"Basic256\", \"Basic128Rsa15\"]).intersection(atuhs):\n",
    "            labs.append(\"opcua:access:weak-policies\")\n",
    "\n",
    "        if mode in (\"None\", \"Sign\"):\n",
    "            labs.append(\"opcua:access:weak-mode\")\n",
    "\n",
    "        return labs\n",
    "                \n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        labs = []\n",
    "        fp = row.get(\"fingerprint\")\n",
    "        for ep in fp.get(\"endpoints\", []):\n",
    "            if l := self.eval_policy(ep[\"policies\"], ep[\"mode\"]):\n",
    "                labs.extend(l)\n",
    "        return labs\n",
    "    \n",
    "class OPCUACertificateClassifier():\n",
    "    def __init__(self):\n",
    "        self.cls = CertificateClassifier()\n",
    "\n",
    "    def label(self, row):\n",
    "        labs = []\n",
    "        eps = row[\"fingerprint\"][\"endpoints\"]\n",
    "        ts = row[\"timestamp\"]\n",
    "        for ep in eps:\n",
    "            if l := self.cls.eval_certificate(ep[\"certificate\"], ts):\n",
    "                labs.extend(l)\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_opcua_flatten(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    opcua_cols = [\"is_opcua\", \"endpoints\", \"discovery_servers\", \"nodes\", \"endpoint_sec_used\", \"rdns\", \"endpoint_cert_test\"]\n",
    "\n",
    "    ndf = (\n",
    "        pd.json_normalize(df[\"result\"], max_level=0)\n",
    "        .rename(columns=lambda x: x.removeprefix(\"result.\"))\n",
    "    )\n",
    "    ndf.drop(ndf.columns.difference(opcua_cols), axis=1, inplace=True)\n",
    "    df = (\n",
    "        pd.concat([df, ndf], axis=1)\n",
    "        .drop(columns=[\"result\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df[\"port\"] = 4840\n",
    "    return df\n",
    "\n",
    "def opcua_tma_loader(scan_id: str, protocol: str) -> pd.DataFrame:\n",
    "    with open(f'data/{scan_id}/protocol={protocol}/{protocol}.jsonl', 'rb') as f:\n",
    "        df = pd.read_json(f, lines=True, encoding='utf-8',)\n",
    "        df = ds.normalize_df(df, protocol)\n",
    "        df = df.drop_duplicates(subset=[\"ip\", \"status\"])\n",
    "        df = s2_opcua_flatten(df)\n",
    "        return df\n",
    "\n",
    "def opcua_filter(df): return df[df.is_opcua.astype(bool)]\n",
    "s2_opcua = classify_records(\n",
    "    'tma-2025', \n",
    "    'opcua',\n",
    "    opcua_filter,\n",
    "    OPCUAIdentifier(),\n",
    "    [OPCUAClassifier(), OPCUACertificateClassifier()],\n",
    "    opcua_tma_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cf6ed9",
   "metadata": {},
   "source": [
    "### RTPS DDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcf693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "@dataclass\n",
    "class Vendor:\n",
    "    v: str\n",
    "    product: str\n",
    "    company: str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"vendor\": self.v,\n",
    "            \"product\": self.product,\n",
    "            \"company\": self.company\n",
    "        }\n",
    "\n",
    "class RTPSIdentifier(Identifier):\n",
    "    vendors = [\n",
    "        Vendor(\"01\", \"RTI Connext DDS\", \"Real-Time Innovations, Inc. (RTI)\"),\n",
    "        Vendor(\"02\", \"OpenSplice DDS\", \"ADLink Ltd.\"),\n",
    "        Vendor(\"03\", \"OpenDDS\", \"Object Computing Inc. (OCI)\"),\n",
    "        Vendor(\"04\", \"Mil-DDS\", \"MilSoft\"),\n",
    "        Vendor(\"05\", \"InterCOM DDS\", \"Kongsberg\"),\n",
    "        Vendor(\"06\", \"CoreDX DDS\", \"Twin Oaks Computing\"),\n",
    "        Vendor(\"07\", \"Not Active\", \"Lakota Technical Solutions, Inc.\"),\n",
    "        Vendor(\"08\", \"Not Active\", \"ICOUP Consulting\"),\n",
    "        Vendor(\"09\", \"Diamond DDS\", \"Electronic and Telecommunication Research Institute (ETRI)\"),\n",
    "        Vendor(\"0a\", \"RTI Connext DDS Micro\", \"Real-Time Innovations, Inc. (RTI)\"),\n",
    "        Vendor(\"0b\", \"Vortex Cafe\", \"ADLink Ltd.\"),\n",
    "        Vendor(\"0c\", \"Not Active\", \"PrismTech Ltd.\"),\n",
    "        Vendor(\"0d\", \"Vortex Lite\", \"ADLink Ltd.\"),\n",
    "        Vendor(\"0e\", \"Qeo (Not Active)\", \"Technicolor\"),\n",
    "        Vendor(\"0f\", \"FastRTPS, FastDDS\", \"eProsima\"),\n",
    "        Vendor(\"10\", \"Eclipse Cyclone DDS\", \"EclipseFondation\"),\n",
    "        Vendor(\"11\", \"GurumDDS\", \"Gurum Networks, Inc.\"),\n",
    "        Vendor(\"12\", \"RustDDS\", \"Atostek\"),\n",
    "        Vendor(\"13\", \"Zhenrong Data Distribution Service (ZRDDS)\", \"Nanjing Zhenrong Software Technology Co.\"),\n",
    "        Vendor(\"14\", \"Dust DDS\", \"S2E Software System B.V.\")\n",
    "    ]\n",
    "\n",
    "    def get_vendor(self, vendor):\n",
    "        for ven in self.vendors:\n",
    "            if ven.v == vendor:\n",
    "                return ven.to_dict()\n",
    "    def parse_response(self, response: str) -> dict:\n",
    "        data = base64.b64decode(response)\n",
    "        if len(data) < 20:\n",
    "            return {}\n",
    "        \n",
    "        magic, version, vendor, host, app, instance = struct.unpack('!4sH2s4s4s4s', data[:20])\n",
    "        vmajor = version >> 8\n",
    "        vminor = version & 0xFF\n",
    "        vendor = format(vendor[1], \"02x\")\n",
    "\n",
    "        fp = {\n",
    "            \"magic\": magic.decode('utf-8', 'ignore'),\n",
    "            \"version\": f\"{vmajor}.{vminor}\",\n",
    "            \"vendor\": vendor,\n",
    "            \"host\": host,\n",
    "            \"app\": app,\n",
    "            \"instance\": instance,\n",
    "            \"product\": None,\n",
    "            \"company\": None,\n",
    "        }\n",
    "\n",
    "        if v := self.get_vendor(vendor):\n",
    "            fp.update(v)\n",
    "\n",
    "        return fp\n",
    "\n",
    "    def fingerprint(self, row):\n",
    "        row[\"fingerprint\"] = self.parse_response(row.get(\"raw_response\", row.get(\"raw-message\")))\n",
    "        return row\n",
    "    \n",
    "class RTPSClassifier(Classifier):\n",
    "    def eval_fingerprint(self, fingerprint):\n",
    "        labs = []\n",
    "        if fingerprint.get(\"magic\") != \"RTPS\":\n",
    "            return\n",
    "\n",
    "        if fingerprint.get(\"vendor\", None) is not None:\n",
    "            labs.append(\"rtps:access:read\")\n",
    "            labs.append(\"rtps:access:leak\")\n",
    "        return labs\n",
    "\n",
    "    def label(self, row) -> list[str]:\n",
    "        labs =[]\n",
    "        if l := self.eval_fingerprint(row.get(\"fingerprint\")):\n",
    "            labs.extend(l)\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce92b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtps_filter(df): return df[df[\"raw-message\"].notna()]\n",
    "s2_rtps = classify_records(\n",
    "    'tma-2025', \n",
    "    'rtps',\n",
    "    rtps_filter,\n",
    "    RTPSIdentifier(),\n",
    "    RTPSClassifier(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6f342",
   "metadata": {},
   "source": [
    "### XMPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e58970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XMPPIdentifier:\n",
    "    certs: pd.DataFrame = None\n",
    "\n",
    "    AUTH_P = re.compile(r\"<auth xmlns=(\\'|\\\"+)(?P<auth>.*?)(\\'|\\\"+)\")\n",
    "    MECHANISMS_P = re.compile(r\"<mechanism>(?P<mech>.*?)<\\/mechanism>\")\n",
    "    TLS_P = re.compile(r\"<starttls xmlns=(\\'|\\\"+)(?P<tls>.*?)(\\'|\\\"+)\")\n",
    "\n",
    "    def __init__(self, certs):\n",
    "        self.certs = certs\n",
    "\n",
    "    def get_certificate(self, ip):\n",
    "        rows = self.certs[self.certs.ip == ip]\n",
    "        return rows.iloc[0]['certificate'] if len(rows) else None\n",
    "\n",
    "    def parse_banner(self, banner: str):\n",
    "        auth = self.AUTH_P.search(banner)\n",
    "        auth = auth.groupdict().get(\"auth\") if auth else None\n",
    "\n",
    "        tls = self.TLS_P.search(banner)\n",
    "        tls = tls.groupdict().get(\"tls\") if tls else None\n",
    "\n",
    "        mechanisms = self.MECHANISMS_P.findall(banner)\n",
    "\n",
    "        return dict(\n",
    "            is_xmpp='jabber:client' in banner,\n",
    "            banner=banner,\n",
    "            authentication=auth,\n",
    "            tls=tls,\n",
    "            mechanisms=mechanisms,\n",
    "        )\n",
    "    \n",
    "    def fingerprint(self, row: pd.Series) -> pd.Series:\n",
    "        banner = self.parse_banner(row.get(\"banner\"))\n",
    "        cert = self.get_certificate(row.get('ip'))\n",
    "        banner.update({'certificate': cert})\n",
    "        row[\"fingerprint\"] = banner\n",
    "        return row\n",
    "    \n",
    "class XMPPClassifier:\n",
    "    def eval_auth(self, fp):\n",
    "        labs = []\n",
    "\n",
    "        if l := self.eval_mechanisms(fp.get(\"mechanisms\")):\n",
    "            labs.extend(l)\n",
    "\n",
    "        # IQ-auth is deprecated, servers must use SASL instead\n",
    "        auth = fp.get(\"authentication\")\n",
    "        if not auth:\n",
    "            return labs\n",
    "\n",
    "        if \"iq-auth\" in auth:\n",
    "            labs.append(\"xmpp:stanza:auth:deprecated\")\n",
    "        return labs\n",
    "\n",
    "    def eval_mechanisms(self, mechanisms: list[str]) -> list[str]:\n",
    "        if not mechanisms:\n",
    "            return []\n",
    "\n",
    "        bad = set([\"PLAIN\", \"DIGEST-MD5\",\"CRAM-MD5\", \"ANONYMOUS\"])\n",
    "        if bad.intersection(mechanisms):\n",
    "            return [\"xmpp:stanza:mechanisms:deprecated\"]\n",
    "\n",
    "    def label(self, row: pd.Series) -> list[str]:\n",
    "        labs = []\n",
    "\n",
    "        fp = row.get(\"fingerprint\")\n",
    "        if l := self.eval_auth(fp):\n",
    "            labs.extend(l)\n",
    "\n",
    "        return labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmpp_filter(df): \n",
    "    return df[\n",
    "        (df.banner.notna()) &\n",
    "        (df.banner != \"\")\n",
    "    ]\n",
    "\n",
    "# ANONYMIZED: omited date of the scan\n",
    "xmpp_date = datetime.strptime(\"XXXX-XX-XX\", \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "xmpp_ds = classify_records(\n",
    "    'tma-2025', \n",
    "    'xmpp',\n",
    "    xmpp_filter,\n",
    "    # ANONYMIZED: this function takes a dataframe with two columns: [ip, certificate]\n",
    "    # This was omited here\n",
    "    XMPPIdentifier(...), \n",
    "    [XMPPClassifier(), CertificateClassifier(xmpp_date)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58208372",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Execute the query on the parquet files using a glob pattern\n",
    "labelled_df = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        protocol, \n",
    "        ip,\n",
    "        labels\n",
    "    FROM read_parquet('data/tma-2025/*/labelled_*.parquet', filename=true)\n",
    "    WHERE labels IS NOT NULL AND len(labels) > 0\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "prepend_if_needed = lambda s, prefix: s if s.startswith(prefix) else prefix + s\n",
    "\n",
    "labelled_df[\"id\"] = labelled_df[\"ip\"].astype(\"category\").cat.codes\n",
    "exp_labdf = labelled_df.explode(\"labels\")\n",
    "exp_labdf[\"label\"] = exp_labdf.apply(lambda x: x[\"labels\"] if x[\"labels\"].startswith(x[\"protocol\"]) else f'{x[\"protocol\"]}:{x[\"labels\"]}', axis=1 )\n",
    "exp_labdf = exp_labdf.sort_values(\"label\")\n",
    "exp_labdf[[\"id\", \"label\"]].to_json(\"labelled.json\", orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
